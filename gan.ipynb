{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","%matplotlib inline\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","from torch import autograd\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset,DataLoader,Subset\n","from PIL import Image,ImageOps,ImageEnhance\n","\n","import cv2\n","import albumentations as A\n","from albumentations.pytorch import ToTensor\n","\n","import glob\n","import xml.etree.ElementTree as ET #for parsing XML\n","import shutil\n","from tqdm import tqdm\n","import time\n","import random\n","\n","\n","import os\n","print(os.listdir(\"../input\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TIME_LIMIT = 32400 - 60*10\n","start_time = time.time()\n","def elapsed_time(start_time):\n","    return time.time() - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#random seeds\n","seed = 2019\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","BATCH_SIZE  = 32\n","NUM_WORKERS = 4\n","EMA = False\n","LABEL_NOISE = False\n","LABEL_NOISE_PROB = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PATH = '../input/all-dogs/all-dogs/'\n","img_filenames = os.listdir(PATH)\n","len(img_filenames)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PATH_ANNOTATION = '../input/annotation/Annotation/'\n","breeds = glob.glob(PATH_ANNOTATION+'*')\n","annotations = []\n","for breed in breeds:\n","    annotations += glob.glob(breed+'/*')\n","len(annotations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["breed_map = {}\n","for annotation in annotations:\n","    breed = annotation.split('/')[-2]\n","    index = breed.split('-')[0]\n","    breed_map.setdefault(index,breed)\n","n_classes = len(breed_map)\n","n_classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["breed_map"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://www.kaggle.com/whizzkid/crop-images-using-bounding-box\n","def bounding_box(img):\n","    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'/'+str(img.split('.')[0])\n","    tree  = ET.parse(bpath)\n","    root  = tree.getroot()\n","    objects = root.findall('object')\n","    bbxs = []\n","    for o in objects:\n","        bndbox = o.find('bndbox') #reading bound box\n","        xmin = int(bndbox.find('xmin').text)\n","        ymin = int(bndbox.find('ymin').text)\n","        xmax = int(bndbox.find('xmax').text)\n","        ymax = int(bndbox.find('ymax').text)\n","        bbxs.append((xmin,ymin,xmax,ymax))\n","    return bbxs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def bounding_box_ratio(img):\n","    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'/'+str(img.split('.')[0])\n","    tree  = ET.parse(bpath)\n","    root  = tree.getroot()\n","    objects = root.findall('object')\n","    bbx_ratios = []\n","    for o in objects:\n","        bndbox = o.find('bndbox') #reading bound box\n","        xmin = int(bndbox.find('xmin').text)\n","        ymin = int(bndbox.find('ymin').text)\n","        xmax = int(bndbox.find('xmax').text)\n","        ymax = int(bndbox.find('ymax').text)\n","        xlen = xmax - xmin\n","        ylen = ymax - ymin\n","        ratio = ylen / xlen\n","        bbx_ratios.append((xlen,ylen,ratio))\n","    return bbx_ratios"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","#threshold for aspect ratio, at the same time idx for each bbx\n","img_filenames_th = []\n","ratios_th = []\n","for img in tqdm(img_filenames):\n","    bbx_ratios = bounding_box_ratio(img)\n","    for i,(xlen,ylen,ratio) in enumerate(bbx_ratios):\n","        if ((ratio>0.2)&(ratio<4.0)):\n","            img_filenames_th.append(img[:-4]+'_'+str(i)+'.jpg')\n","            ratios_th.append(ratio)\n","ratios_th = np.array(ratios_th)\n","\n","print('original : ', len(img_filenames))\n","print('after th : ', len(img_filenames_th))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#from https://www.kaggle.com/korovai/dogs-images-intruders-extraction\n","intruders = [\n","    #n02088238-basset\n","    'n02088238_10870_0.jpg',\n","    \n","    #n02088466-bloodhound\n","    'n02088466_6901_1.jpg',\n","    'n02088466_6963_0.jpg',\n","    'n02088466_9167_0.jpg',\n","    'n02088466_9167_1.jpg',\n","    'n02088466_9167_2.jpg',\n","    \n","    #n02089867-Walker_hound\n","    'n02089867_2221_0.jpg',\n","    'n02089867_2227_1.jpg',\n","    \n","    #n02089973-English_foxhound # No details\n","    'n02089973_1132_3.jpg',\n","    'n02089973_1352_3.jpg',\n","    'n02089973_1458_1.jpg',\n","    'n02089973_1799_2.jpg',\n","    'n02089973_2791_3.jpg',\n","    'n02089973_4055_0.jpg',\n","    'n02089973_4185_1.jpg',\n","    'n02089973_4185_2.jpg',\n","    \n","    #n02090379-redbone\n","    'n02090379_4673_1.jpg',\n","    'n02090379_4875_1.jpg',\n","    \n","    #n02090622-borzoi # Confusing\n","    'n02090622_7705_1.jpg',\n","    'n02090622_9358_1.jpg',\n","    'n02090622_9883_1.jpg',\n","    \n","    #n02090721-Irish_wolfhound # very small\n","    'n02090721_209_1.jpg',\n","    'n02090721_1222_1.jpg',\n","    'n02090721_1534_1.jpg',\n","    'n02090721_1835_1.jpg',\n","    'n02090721_3999_1.jpg',\n","    'n02090721_4089_1.jpg',\n","    'n02090721_4276_2.jpg',\n","    \n","    #n02091032-Italian_greyhound\n","    'n02091032_722_1.jpg',\n","    'n02091032_745_1.jpg',\n","    'n02091032_1773_0.jpg',\n","    'n02091032_9592_0.jpg',\n","    \n","    #n02091134-whippet\n","    'n02091134_2349_1.jpg',\n","    'n02091134_14246_2.jpg',\n","    \n","    #n02091244-Ibizan_hound\n","    'n02091244_583_1.jpg',\n","    'n02091244_2407_0.jpg',\n","    'n02091244_3438_1.jpg',\n","    'n02091244_5639_1.jpg',\n","    'n02091244_5639_2.jpg',\n","    \n","    #n02091467-Norwegian_elkhound\n","    'n02091467_473_0.jpg',\n","    'n02091467_4386_1.jpg',\n","    'n02091467_4427_1.jpg',\n","    'n02091467_4558_1.jpg',\n","    'n02091467_4560_1.jpg',\n","    \n","    #n02091635-otterhound\n","    'n02091635_1192_1.jpg',\n","    'n02091635_4422_0.jpg',\n","    \n","    #n02091831-Saluki\n","    'n02091831_1594_1.jpg',\n","    'n02091831_2880_0.jpg',\n","    'n02091831_7237_1.jpg',\n","    \n","    #n02092002-Scottish_deerhound\n","    'n02092002_1551_1.jpg',\n","    'n02092002_1937_1.jpg',\n","    'n02092002_4218_0.jpg',\n","    'n02092002_4596_0.jpg',\n","    'n02092002_5246_1.jpg',\n","    'n02092002_6518_0.jpg',\n","    \n","    #02093256-Staffordshire_bullterrier\n","    'n02093256_1826_1.jpg',\n","    'n02093256_4997_0.jpg',\n","    'n02093256_14914_0.jpg',\n","    \n","    #n02093428-American_Staffordshire_terrier\n","    'n02093428_5662_0.jpg',\n","    'n02093428_6949_1.jpg'\n","            ]\n","\n","len(intruders)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def data_preprocessing(img_path,bbx_idx):\n","    bbx = bounding_box(img_path)[bbx_idx]\n","    img  = Image.open(os.path.join(PATH,img_path))#PILImage format\n","    img_cropped  = img.crop(bbx)\n","    return img_cropped"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","breed_map_2 = {}\n","for i,b in enumerate(breed_map.keys()):\n","    breed_map_2[b] = i"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DogDataset(Dataset):\n","    def __init__(self, path, img_list, transform1=None, transform2=None):\n","        self.path      = path\n","        self.img_list  = img_list\n","        self.transform1 = transform1\n","        self.transform2 = transform2\n","        \n","        self.imgs   = []\n","        self.labels = []\n","        for i,full_img_path in enumerate(self.img_list):\n","            if full_img_path in intruders:\n","                continue\n","            #img\n","            img_path = full_img_path[:-6]+'.jpg'\n","            bbx_idx  = int(full_img_path[-5])\n","            img = data_preprocessing(img_path,bbx_idx)\n","            if self.transform1:\n","                img = self.transform1(img) #output shape=(ch,h,w)\n","            self.imgs.append(img)\n","            #label\n","            label = breed_map_2[img_path.split('_')[0]]\n","            self.labels.append(label)\n","            \n","    def __len__(self):\n","        return len(self.imgs)\n","    \n","    def __getitem__(self,idx):\n","        img = self.imgs[idx]\n","        if self.transform2:\n","            img = self.transform2(img)\n","        label = self.labels[idx]\n","        return {'img':img, 'label':label}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","# generate 64x64 images!\n","#resize_size = 68\n","img_size    = 64\n","batch_size  = BATCH_SIZE\n","MEAN1,MEAN2,MEAN3 = 0.5, 0.5, 0.5\n","STD1,STD2,STD3    = 0.5, 0.5, 0.5\n","\n","transform1 = transforms.Compose([transforms.Resize(img_size)])\n","\n","transform2 = transforms.Compose([transforms.RandomCrop(img_size),\n","                                 #transforms.RandomAffine(degrees=5),\n","                                 transforms.RandomHorizontalFlip(p=0.5),\n","                                 #transforms.RandomApply(random_transforms, p=0.3),\n","                                 transforms.ToTensor(),\n","                                 transforms.Normalize(mean=[MEAN1, MEAN2, MEAN3],\n","                                                      std=[STD1, STD2, STD3]),\n","                                ])\n","\n","train_set = DogDataset(path=PATH,\n","                       img_list=img_filenames_th,\n","                       transform1=transform1,\n","                       transform2=transform2,\n","                      )\n","\n","train_loader = DataLoader(train_set,\n","                          shuffle=True, batch_size=batch_size,\n","                          num_workers=NUM_WORKERS, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train_set)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img = data_preprocessing(img_filenames_th[1500][:-6]+'.jpg',0)\n","img = transform1(img)\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def conv3x3(in_channel, out_channel): #not change resolusion\n","    return nn.Conv2d(in_channel,out_channel,\n","                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n","\n","def conv1x1(in_channel, out_channel): #not change resolution\n","    return nn.Conv2d(in_channel,out_channel,\n","                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n","\n","def init_weight(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.orthogonal_(m.weight, gain=1)\n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","            \n","    elif classname.find('Batch') != -1:\n","        m.weight.data.normal_(1,0.02)\n","        m.bias.data.zero_()\n","    \n","    elif classname.find('Linear') != -1:\n","        nn.init.orthogonal_(m.weight, gain=1)\n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","    \n","    elif classname.find('Embedding') != -1:\n","        nn.init.orthogonal_(m.weight, gain=1)\n","        \n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.channels = channels\n","        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n","        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n","        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels//2)).apply(init_weight)\n","        self.o        = nn.utils.spectral_norm(conv1x1(channels//2, channels)).apply(init_weight)\n","        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n","        \n","    def forward(self, inputs):\n","        batch,c,h,w = inputs.size()\n","        theta = self.theta(inputs) #->(*,c/8,h,w)\n","        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c/8,h/2,w/2)\n","        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c/2,h/2,w/2)\n","        \n","        theta = theta.view(batch, self.channels//8, -1) #->(*,c/8,h*w)\n","        phi   = phi.view(batch, self.channels//8, -1) #->(*,c/8,h*w/4)\n","        g     = g.view(batch, self.channels//2, -1) #->(*,c/2,h*w/4)\n","        \n","        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w/4)\n","        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels//2,h,w)) #->(*,c,h,w)\n","        return self.gamma*o + inputs\n","        \n","    \n","class ConditionalNorm(nn.Module):\n","    def __init__(self, in_channel, n_condition):\n","        super().__init__()\n","        self.bn = nn.BatchNorm2d(in_channel, affine=False) #no learning parameters\n","        self.embed = nn.Linear(n_condition, in_channel* 2)\n","        \n","        nn.init.orthogonal_(self.embed.weight.data[:, :in_channel], gain=1)\n","        self.embed.weight.data[:, in_channel:].zero_()\n","\n","    def forward(self, inputs, label):\n","        out = self.bn(inputs)\n","        embed = self.embed(label.float())\n","        gamma, beta = embed.chunk(2, dim=1)\n","        gamma = gamma.unsqueeze(2).unsqueeze(3)\n","        beta = beta.unsqueeze(2).unsqueeze(3)\n","        out = gamma * out + beta\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#BigGAN + leaky_relu           \n","class ResBlock_G(nn.Module):\n","    def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n","        super().__init__()\n","        self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n","        self.upsample = nn.Sequential()\n","        if upsample:\n","            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n","        self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n","        self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n","        self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n","        self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n","        \n","    def forward(self, inputs, condition):\n","        x  = F.leaky_relu(self.cbn1(inputs, condition))\n","        x  = self.upsample(x)\n","        x  = self.conv3x3_1(x)\n","        x  = self.conv3x3_2(F.leaky_relu(self.cbn2(x, condition)))\n","        x += self.conv1x1(self.upsample(inputs)) #shortcut\n","        return x\n","\n","class Generator(nn.Module):\n","    def __init__(self, n_feat, codes_dim=24, n_classes=n_classes):\n","        super().__init__()\n","        self.fc   = nn.Sequential(\n","            nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight)\n","        )\n","        self.res1 = ResBlock_G(16*n_feat, 16*n_feat, codes_dim+n_classes, upsample=True)\n","        self.res2 = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n","        #self.attn2 = Attention(8*n_feat)\n","        self.res3 = ResBlock_G( 8*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n","        self.attn = Attention(4*n_feat)\n","        self.res4 = ResBlock_G( 4*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n","        self.conv = nn.Sequential(\n","            #nn.BatchNorm2d(2*n_feat).apply(init_weight),\n","            nn.LeakyReLU(),\n","            nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n","        )\n","        \n","    def forward(self, z, label_ohe, codes_dim=24):\n","        '''\n","        z.shape = (*,120)\n","        label_ohe.shape = (*,n_classes)\n","        '''\n","        batch = z.size(0)\n","        z = z.squeeze()\n","        label_ohe = label_ohe.squeeze()\n","        codes = torch.split(z, codes_dim, dim=1)\n","        \n","        x = self.fc(codes[0]) #->(*,16ch*4*4)\n","        x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n","        \n","        condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n","        x = self.res1(x, condition)#->(*,16ch,8,8)\n","        \n","        condition = torch.cat([codes[2], label_ohe], dim=1)\n","        x = self.res2(x, condition) #->(*,8ch,16,16)\n","        #x = self.attn2(x) #not change shape\n","        \n","        condition = torch.cat([codes[3], label_ohe], dim=1)\n","        x = self.res3(x, condition) #->(*,4ch,32,32)\n","        x = self.attn(x) #not change shape\n","        \n","        condition = torch.cat([codes[4], label_ohe], dim=1)\n","        x = self.res4(x, condition) #->(*,2ch,64,64)\n","        \n","        x = self.conv(x) #->(*,3,64,64)\n","        x = torch.tanh(x)\n","        return x\n","    \n","\n","class ResBlock_D(nn.Module):\n","    def __init__(self, in_channel, out_channel, downsample=True):\n","        super().__init__()\n","        self.layer = nn.Sequential(\n","            nn.LeakyReLU(0.2),\n","            nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n","            nn.LeakyReLU(0.2),\n","            nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n","        )\n","        self.shortcut = nn.Sequential(\n","            nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n","        )\n","        if downsample:\n","            self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n","            self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n","        \n","    def forward(self, inputs):\n","        x  = self.layer(inputs)\n","        x += self.shortcut(inputs)\n","        return x\n","    \n","\n","class Discriminator(nn.Module):\n","    def __init__(self, n_feat, n_classes=n_classes):\n","        super().__init__()\n","        self.res1 = ResBlock_D(3, n_feat, downsample=True)\n","        self.attn = Attention(n_feat)\n","        self.res2 = ResBlock_D(  n_feat, 2*n_feat, downsample=True)\n","        #self.attn2 = Attention(2*n_feat)\n","        self.res3 = ResBlock_D(2*n_feat, 4*n_feat, downsample=True)\n","        self.res4 = ResBlock_D(4*n_feat, 8*n_feat, downsample=True)\n","        self.res5 = ResBlock_D(8*n_feat,16*n_feat, downsample=False)\n","        self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n","        self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n","        \n","    def forward(self, inputs, label):\n","        batch = inputs.size(0) #(*,3,64,64)\n","        h = self.res1(inputs) #->(*,ch,32,32)\n","        h = self.attn(h) #not change shape\n","        h = self.res2(h) #->(*,2ch,16,16)\n","        #h = self.attn2(h) #not change shape\n","        h = self.res3(h) #->(*,4ch,8,8)\n","        h = self.res4(h) #->(*,8ch,4,4)\n","        h = self.res5(h) #->(*,16ch,4,4)\n","        h = torch.sum((F.leaky_relu(h,0.2)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n","        outputs = self.fc(h) #->(*,1)\n","        \n","        if label is not None:\n","            embed = self.embedding(label) #->(*,16ch)\n","            outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n","        \n","        outputs = torch.sigmoid(outputs)\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_img(netG,fixed_noise,fixed_aux_labels=None):\n","    if fixed_aux_labels is not None:\n","        gen_image = netG(fixed_noise,fixed_aux_labels).to('cpu').clone().detach().squeeze(0)\n","    else:\n","        gen_image = netG(fixed_noise).to('cpu').clone().detach().squeeze(0)\n","    #denormalize\n","    gen_image = gen_image*0.5 + 0.5\n","    gen_image_numpy = gen_image.numpy().transpose(0,2,3,1)\n","    return gen_image_numpy\n","\n","def show_generate_imgs(netG,fixed_noise,fixed_aux_labels=None):\n","    gen_images_numpy = generate_img(netG,fixed_noise,fixed_aux_labels)\n","\n","    fig = plt.figure(figsize=(25, 16))\n","    # display 10 images from each class\n","    for i, img in enumerate(gen_images_numpy):\n","        ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n","        plt.imshow(img)\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://www.kaggle.com/osciiart/resnet34-mel-ver3-log-multi-hardaug?scriptVersionId=13887036\n","def cycle(iterable):\n","    \"\"\"\n","    dataloaderをiteratorに変換\n","    :param iterable:\n","    :return:\n","    \"\"\"\n","    while True:\n","        for x in iterable:\n","            yield x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#BigGAN\n","def run(lr_G=3e-4,lr_D=3e-4, beta1=0.0, beta2=0.999, nz=120, epochs=2, \n","        n_ite_D=1, ema_decay_rate=0.999, show_epoch_list=None, output_freq=10):\n","\n","    netG = Generator(n_feat=36, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n","    netD = Discriminator(n_feat=42, n_classes=n_classes).to(device)\n","\n","    if EMA:\n","        #EMA of G for sampling\n","        netG_EMA = Generator(n_feat=42, codes_dim=24, n_classes=n_classes).to(device)\n","        netG_EMA.load_state_dict(netG.state_dict())\n","        for p in netG_EMA.parameters():\n","            p.requires_grad = False\n","\n","        \n","    print(count_parameters(netG))\n","    print(count_parameters(netD))\n","    \n","    real_label = 0.9\n","    fake_label = 0\n","    \n","    D_loss_list = []\n","    G_loss_list = []\n","    \n","    dis_criterion = nn.BCELoss().to(device)\n","\n","    optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(beta1, beta2))\n","    optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, beta2))\n","    \n","    fixed_noise = torch.randn(32, nz, 1, 1, device=device)\n","    #fixed_noise = fixed_noise / fixed_noise.norm(dim=1, keepdim=True)\n","    fixed_aux_labels     = np.random.randint(0,n_classes, 32)\n","    fixed_aux_labels_ohe = np.eye(n_classes)[fixed_aux_labels]\n","    fixed_aux_labels_ohe = torch.from_numpy(fixed_aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n","    fixed_aux_labels_ohe = fixed_aux_labels_ohe.float().to(device, non_blocking=True)\n","\n","    netG.train()\n","    netD.train()\n","\n","    ### training here\n","    for epoch in range(1,epochs+1):\n","        if elapsed_time(start_time) > TIME_LIMIT:\n","            print(f'elapsed_time go beyond {TIME_LIMIT} sec')\n","            break\n","        D_running_loss = 0\n","        G_running_loss = 0\n","        for ii, data in enumerate(train_loader):\n","            for _ in range(n_ite_D):\n","                \n","                if LABEL_NOISE:\n","                    real_label = 0.9\n","                    fake_label = 0\n","                    if np.random.random() < LABEL_NOISE_PROB:\n","                        real_label = 0\n","                        fake_label = 0.9\n","                    \n","                # train with real\n","                netD.zero_grad()\n","                real_images = data['img'].to(device, non_blocking=True) \n","                batch_size  = real_images.size(0)\n","                dis_labels  = torch.full((batch_size, 1), real_label, device=device) #shape=(*,)\n","                aux_labels  = data['label'].long().to(device, non_blocking=True) #shape=(*,)\n","                dis_output = netD(real_images, aux_labels) #dis shape=(*,1)\n","                errD_real  = dis_criterion(dis_output, dis_labels)\n","                errD_real.backward(retain_graph=True)\n","\n","                # train with fake\n","                noise  = torch.randn(batch_size, nz, 1, 1, device=device)\n","                #noise = noise / noise.norm(dim=1, keepdim=True)\n","                aux_labels     = np.random.randint(0,n_classes, batch_size)\n","                aux_labels_ohe = np.eye(n_classes)[aux_labels]\n","                aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n","                aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n","                aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n","                \n","                fake = netG(noise, aux_labels_ohe) #output shape=(*,3,64,64)\n","                dis_labels.fill_(fake_label)\n","                dis_output = netD(fake.detach(),aux_labels)\n","                errD_fake  = dis_criterion(dis_output, dis_labels)\n","                errD_fake.backward(retain_graph=True)\n","                D_running_loss += (errD_real.item() + errD_fake.item())/len(train_loader)\n","                optimizerD.step()\n","\n","            netG.zero_grad()\n","            dis_labels.fill_(real_label)  # fake labels are real for generator cost\n","            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n","            aux_labels     = np.random.randint(0,n_classes, batch_size)\n","            aux_labels_ohe = np.eye(n_classes)[aux_labels]\n","            aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n","            aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n","            aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n","            fake  = netG(noise, aux_labels_ohe)\n","            \n","            dis_output = netD(fake, aux_labels)\n","            errG   = dis_criterion(dis_output, dis_labels)\n","            errG.backward(retain_graph=True)\n","            G_running_loss += errG.item()/len(train_loader)\n","            optimizerG.step()\n","        \n","        if EMA:\n","            #update netG_EMA\n","            param_itr = cycle(netG.parameters())\n","            for i,p_EMA in enumerate(netG_EMA.parameters()):\n","                p = next(param_itr)\n","                p_EMA.data = (1-ema_decay_rate)*p_EMA.data + ema_decay_rate*p.data\n","                p_EMA.requires_grad = False\n","        \n","        #log\n","        D_loss_list.append(D_running_loss)\n","        G_loss_list.append(G_running_loss)\n","        \n","        #output\n","        if epoch % output_freq == 0:\n","            print('[{:d}/{:d}] D_loss = {:.3f}, G_loss = {:.3f}, elapsed_time = {:.1f} min'.format(epoch,epochs,D_running_loss,G_running_loss,elapsed_time(start_time)/60))\n","            \n","        if epoch in show_epoch_list:\n","            print('epoch = {}'.format(epoch))\n","            if not EMA:\n","                show_generate_imgs(netG,fixed_noise,fixed_aux_labels_ohe)\n","            elif EMA:\n","                show_generate_imgs(netG_EMA,fixed_noise,fixed_aux_labels_ohe)\n","            \n","        if epoch % 100 == 0:\n","            if not EMA:\n","                torch.save(netG.state_dict(), f'generator_epoch{epoch}.pth')\n","            elif EMA:\n","                torch.save(netG_EMA.state_dict(), f'generator_epoch{epoch}.pth')\n","    \n","    if not EMA:\n","        torch.save(netG.state_dict(), 'generator.pth')\n","    elif EMA:\n","        torch.save(netG_EMA.state_dict(), 'generator.pth')\n","    torch.save(netD.state_dict(), 'discriminator.pth')\n","    \n","    res = {'netG':netG,\n","           'netD':netD,\n","           'nz':nz,\n","           'fixed_noise':fixed_noise,\n","           'fixed_aux_labels_ohe':fixed_aux_labels_ohe,\n","           'D_loss_list':D_loss_list,\n","           'G_loss_list':G_loss_list,\n","          }\n","    if EMA:\n","        res['netG_EMA'] = netG_EMA\n","        \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","#show_epoch_list = np.arange(0,100,1)\n","show_epoch_list = np.arange(0,500+10,10)\n","\n","res = run(lr_G=3e-4,lr_D=3e-4, beta1=0.0, beta2=0.999, nz=120, epochs=500, \n","          n_ite_D=1, ema_decay_rate=None, show_epoch_list=show_epoch_list, output_freq=10)\n","# res = run(lr_G=3e-4,lr_D=3e-4, beta1=0.5, beta2=0.999, nz=120, epochs=500, \n","#           n_ite_D=1, ema_decay_rate=None, show_epoch_list=show_epoch_list, output_freq=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(res['D_loss_list'], label='D_loss')\n","plt.plot(res['G_loss_list'], label='G_loss')\n","plt.grid()\n","plt.legend()\n","plt.title('loss history');"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#truncation_trick\n","def submission_generate_images(res, truncated=None):\n","    im_batch_size=50\n","    n_images=10000\n","    if not EMA:\n","        netG = res['netG']\n","    elif EMA:\n","        netG = res['netG_EMA']\n","    nz   = res['nz']\n","    if not os.path.exists('../output_images'):\n","        os.mkdir('../output_images')\n","    for i_batch in range(0, n_images, im_batch_size):\n","        if truncated is not None:\n","            flag = True\n","            while flag:\n","                z = np.random.randn(100*im_batch_size*nz)\n","                z = z[np.where(abs(z)<truncated)]\n","                if len(z)>=im_batch_size*nz:\n","                    flag=False\n","            gen_z = torch.from_numpy(z[:im_batch_size*nz]).view(im_batch_size,nz,1,1)\n","            gen_z = gen_z.float().to(device)\n","        else:\n","            gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n","        aux_labels     = np.random.randint(0,n_classes, im_batch_size)\n","        aux_labels_ohe = np.eye(n_classes)[aux_labels]\n","        aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n","        aux_labels_ohe = aux_labels_ohe.float().to(device)\n","        \n","        gen_images = netG(gen_z,aux_labels_ohe)\n","        gen_images = gen_images.to(\"cpu\").clone().detach() \n","        gen_images = gen_images*0.5 + 0.5\n","        for i_image in range(gen_images.size(0)):\n","            save_image(gen_images[i_image, :, :, :],\n","                       os.path.join(f'../output_images', f'image_{i_batch+i_image:05d}.png'))\n","    shutil.make_archive(f'images', 'zip', f'../output_images')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","submission_generate_images(res,truncated=0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not EMA:\n","    gen_image_numpy = generate_img(res['netG'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\n","elif EMA:\n","    gen_image_numpy = generate_img(res['netG_EMA'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\n","for img in gen_image_numpy:\n","    plt.imshow(img)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["elapsed_time(start_time)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
